import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD
import java.util.{Date, Locale}
import java.util.Calendar
import java.text.DateFormat
import java.text.SimpleDateFormat
import java.sql.Date
import com.databricks.spark.xml
import org.apache.hadoop.io.LongWritable
import org.apache.hadoop.io.Text
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import scala.util.Try
//import org.apache.spark.sql.DataFrame

import org.apache.hadoop.fs.Path
import org.apache.hadoop.fs.FileUtil
import org.apache.hadoop.fs._

//case class BirthData(tag: String, eid: String, start_weight:Double,start_date: String)

case class BirthData(eid: String, start_weight:Double,start_date: String)
case class Image(tag: String, imageName: String)

object BeefModel {

  def main(args: Array[String]) {
val sparkConf = new SparkConf().setAppName("beef");
val sc = new SparkContext(sparkConf);
//val rootLogger = Logger.getRootLogger()
//rootLogger.setLevel(Level.ERROR)
val keyId = args(0);
val secretKey = args(1);

sc.hadoopConfiguration.set("fs.s3.awsSecretAccessKey",secretKey);
sc.hadoopConfiguration.set("fs.s3.awsAccessKeyId",keyId);
sc.hadoopConfiguration.set("fs.s3n.awsAccessKeyId",keyId)
sc.hadoopConfiguration.set("fs.s3n.awsSecretAccessKey",secretKey)
sc.hadoopConfiguration.set("fs.s3n.impl", "org.apache.hadoop.fs.s3native.NativeS3FileSystem")
sc.hadoopConfiguration.set("fs.s3.impl", "org.apache.hadoop.fs.s3native.NativeS3FileSystem")
//sc.hadoopConfiguration.set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")


val hadoopConf = sc.hadoopConfiguration;
//val fs = FileSystem.get(new java.net.URI("file:///home/cloudera/agriBeef/"), hadoopConf)
//val srcPath = targetPath
//val dstPath = targetPath1

//val ssc = new StreamingContext(sc, Seconds(30))
val sqlContext = new SQLContext(sc);
import sqlContext.implicits._;
def hasPath(path: String) = Try(sqlContext.read.json(path)).isSuccess;
def hasColumn(df: DataFrame, path: String) = Try(df(path)).isSuccess;
val format = new java.text.SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss");
val format1 = new java.text.SimpleDateFormat("dd/MM/yyyy");


val customschema = StructType(Array(StructField("ads:tag", StringType,nullable=true), StructField("ads:eid",StringType,nullable=false),StructField("ads:group",LongType,nullable=true),StructField("ads:sex",StringType,nullable=true),StructField("ads:dob",StringType,nullable=true),StructField("ads:breed",StringType,nullable=true),StructField("ads:colour",StringType,nullable=true),StructField("ads:sire",StringType,nullable=true),StructField("ads:dam",StringType,nullable=true),StructField("ads:originalTag",StringType,nullable=true),StructField("ads:originalLocationId",StringType,nullable=true),StructField("ads:locationFarm",StringType,nullable=true),StructField("ads:locationProperty",StringType,true),StructField("ads:locationArea",StringType,nullable=true),StructField("ads:donorDam",StringType,nullable=true),StructField("ads:status",StringType,nullable=true)));
val stream = getClass.getResourceAsStream("/props.csv")
val lines = scala.io.Source.fromInputStream(stream).getLines
val lineArray = lines.toArray
val paths = lineArray(0).split(",").map(_.trim)
val animalPath = paths(0)
val feedPath = paths(1)
val weightsPath = paths(2);
val stPath = paths(3);
val farmPath = paths(4);
val imPath = paths(5);
val targetPath1 = paths(6);
val targetPath = "/user/process/";

val ctime = System.currentTimeMillis();
val tempimPath = "/user/images/"+s"$ctime"

val imdata = sqlContext.read.format("com.databricks.spark.xml").option("rowTag", "Details").load(imPath);
imdata.rdd.saveAsTextFile(tempimPath);
val imText = sc.textFile(tempimPath);
val imageRDD = imText.map(_.split(",")).map(t=>(t(12),t(144)));
val imageDF_b_cache = imageRDD.map{case(a,b) => Image(a,b)}.toDF();
val imageDF = imageDF_b_cache.cache();
val loaddata1_b_cache = sqlContext.read.format("com.databricks.spark.xml").option("rowTag", "ads:animal").schema(customschema).load(animalPath);
val loaddata1 = loaddata1_b_cache.cache();

val loaddata2 = loaddata1.withColumn("Farm",lit(farmPath));

val cowDF = loaddata2.select(($"ads:eid").as("eid"),$"ads:group".as("ID_DTM_Pen"),$"Farm",$"ads:tag".as("tag"),struct(($"ads:breed").as("breed"),($"ads:colour").as("colour"),($"ads:dam").as("dam"),($"ads:dob").as("dob"),($"ads:donorDam").as("donorDam"),($"ads:group").as("group"),($"ads:locationArea").as("locationArea"),($"ads:locationFarm").as("locationFarm"),($"ads:locationProperty").as("locationProperty"),($"ads:originalLocationId").as("originalLocationId"),($"ads:originalTag").as("originalTag"),($"ads:sex").as("sex"),($"ads:sire").as("sire"),($"ads:status").as("status")).as("params"));

val tagDF = cowDF.select($"eid",$"tag");

val imageTagDF = tagDF.join(imageDF,"tag");
val imageTagDFMod = imageTagDF.select($"eid",$"imageName");

val feedDf_b_cache = sqlContext.read.json(feedPath);
val feedDf = feedDf_b_cache.cache();


val joinDF = cowDF.join(feedDf,Seq("ID_DTM_Pen","Farm"),"left_outer");

val joinDFMod_im = joinDF.select($"eid",$"tag",struct(($"ID_DTM_Pen").as("group"),$"Farm", $"params",$"ChemicalParameters",$"Cows",$"ExecutionDate",$"FeedoutDate",$"ID_DTM_Recipe",$"ID_Load",$"ID_Session",$"Ingredients",$"PenDescription",$"RecipeMajorRevision",$"RecipeMinorRevision",$"RecipeName").as("FeedNMaster"));

val joinDFMod_wim =  joinDFMod_im.join(imageTagDFMod,Seq("eid"),"left_outer");

val joinDFMod = joinDFMod_im.cache();

val weightDF_b_cache = sqlContext.read.format("com.databricks.spark.xml").option("rowTag", "ads:animal").option("attributePrefix","@").option("valueTag","value").load(weightsPath);
val weightDF = weightDF_b_cache.cache();

val weightDFMod = weightDF.select(($"ads:animalId.ads:eid").as("eid"),($"ads:datetime").as("datetime"),struct(($"ads:weight.@recorded").as("@recorded"),(($"ads:weight.value").cast(DoubleType)).as("value")).alias("weight"));


val start_data_b_cache = sc.textFile(stPath);
val start_data = start_data_b_cache.cache();
val start_data_mod = start_data.map(_.split(",")).map(t=>(t(1),t(2),t(11).toDouble,t(12)));

import sqlContext.implicits._

val birthdata_b_cache = start_data_mod.map{case (a,b,c,d) => BirthData(b,c,d)}.toDF();
val birthdata = birthdata_b_cache.cache();

val wightDFJoined = weightDFMod.join(birthdata,Seq("eid"),"left_outer")

val finalDF_b_cache = wightDFJoined.join(joinDFMod,Seq("eid"),"left_outer")

val finalDF = finalDF_b_cache.cache();


val s = finalDF.select($"eid").map(_.toString()).collect

val sfs = FileSystem.get(hadoopConf)
val dfs = FileSystem.get(new java.net.URI(targetPath1), hadoopConf)
val srcPath = new Path(targetPath)
 
val dstPath = new Path(targetPath1)


s.foreach{a => val ss = a.dropRight(1); 
val sx = ss.slice(1,ss.length);
val filterDF = finalDF.filter($"eid".contains(sx));
val str = "/user/proc/"+s"$sx";
val wgain =0; val avgGain = 0; 
//var image_id = "No Image";
val dstPath_str = targetPath1+s"$sx"+s"$ctime";
val srcPath_str = targetPath+s"$sx"+s"$ctime";
val srcPath = new Path(srcPath_str);
val dstPath = new Path(dstPath_str);

//println("ImageID  "+image_id);
if (hasPath(str)) {val wpa = sqlContext.read.json(str);val wpaw = wpa.select(($"weight.value").as("weight"));

//println("ImageID  "+image_id);

val wpar = wpa.select(($"weight.@recorded").as("rtime"));
val pweight = wpaw.first().getDouble(0);
val prtime = wpar.first().getString(0); 
val wcaw = filterDF.select(($"weight.value").as("weight"));val wcar = filterDF.select(($"weight.@recorded").as("rtime")); 
val cweight = wcaw.first().getDouble(0); 
val crtime = wcar.first().getString(0); 
val daysdiff = ((format.parse(crtime).getTime() - format.parse(prtime).getTime())/(1000*60*60*24)).toDouble;
val wgain = (cweight-pweight); val avgGain = (wgain.toDouble/daysdiff.toDouble);val wgDF=filterDF.withColumn("WeightGain",lit(wgain));val avgDF=wgDF.withColumn("AvgGain",lit(avgGain)); 
val imDF = imageTagDFMod.filter($"eid".contains(sx));

if(imDF.count == 1){
val imDF_im = imDF.select($"imageName");
val image_id = imDF_im.first().getString(0);
val avgDFWImage = avgDF.withColumn("Image",lit(image_id));  
avgDFWImage.write.mode("append").json(srcPath_str) ; 
FileUtil.copyMerge(sfs, srcPath,dfs,dstPath,true,hadoopConf,null);
 
}

if(imDF.count == 0){
val image_id = "No Image";
val avgDFWImage = avgDF.withColumn("Image",lit(image_id));  
//println("ImageID  "+image_id); 
avgDFWImage.write.mode("append").json(srcPath_str) ; 
FileUtil.copyMerge(sfs, srcPath,dfs,dstPath,true,hadoopConf,null);

};
 }  


if (!hasPath(str)){val startFilter = birthdata.filter($"eid".contains(sx)); val wpaw = startFilter.select($"start_weight"); val pweight = wpaw.first().getDouble(0); val wpar = startFilter.select($"start_date"); val prtime = wpar.first().getString(0); val wcaw = filterDF.select(($"weight.value").as("weight"));val wcar = filterDF.select(($"weight.@recorded").as("rtime"));val cweight = wcaw.first().getDouble(0); val crtime = wcar.first().getString(0);  val daysdiff = ((format.parse(crtime).getTime() - format1.parse(prtime).getTime())/(1000*60*60*24)).toDouble; val wgain = (cweight-pweight); val avgGain = (wgain.toDouble/daysdiff.toDouble); val wgDF=filterDF.withColumn("WeightGain",lit(wgain));
val avgDF=wgDF.withColumn("AvgGain",lit(avgGain));

val imDF = imageTagDFMod.filter($"eid".contains(sx));

if(imDF.count == 1){
val imDF_im = imDF.select($"imageName");
val image_id = imDF_im.first().getString(0);
val avgDFWImage = avgDF.withColumn("Image",lit(image_id));  
//println("ImageID  "+image_id); 
avgDFWImage.write.mode("append").json(srcPath_str) ; 
FileUtil.copyMerge(sfs, srcPath,dfs,dstPath,true,hadoopConf,null);
}

if(imDF.count == 0){
val image_id = "No Image";
val avgDFWImage = avgDF.withColumn("Image",lit(image_id));  
//println("ImageID  "+image_id); 
avgDFWImage.write.mode("append").json(srcPath_str) ; 
FileUtil.copyMerge(sfs, srcPath,dfs,dstPath,true,hadoopConf,null);

 } ; 

};

}

val s1 = weightDFMod.select($"eid").map(_.toString()).collect

s1.foreach{a => val ss = a.dropRight(1); val sx = ss.slice(1,ss.length);val filterDF = weightDFMod.filter($"eid".contains(sx));val str = "/user/proc/"+s"$sx"; filterDF.write.mode("overwrite").json(str) }

//val hadoopConf = sc.hadoopConfiguration
//val sfs = FileSystem.get(hadoopConf)
//val dfs = FileSystem.get(new java.net.URI("s3://testene/jswebbProc/"), hadoopConf)
//val srcPath = new Path(targetPath)
//val dstPath = new Path(targetPath1)
//FileUtil.copyMerge(sfs, srcPath,dfs,dstPath,true,hadoopConf,null)
}
}



